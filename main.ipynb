{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout, Flatten, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import datetime\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set globals\n",
    "max_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df_x and df_y from the api calls and the label data\n",
    "df_x = pd.read_csv(\"data/all_call_data.txt\", header=None)\n",
    "df_y = pd.read_csv(\"data/all_labels.csv\", header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categories in target to use integers\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_df_y = label_encoder.fit_transform(df_y.iloc[:, 0])\n",
    "\n",
    "# holds the label encoding used for the datasets\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "inversed_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count for the total vocab\n",
    "concatenated_string = ' '.join(df_x[0])\n",
    "all_strings = concatenated_string.split()\n",
    "unique_count = len(set(all_strings))\n",
    "print(\"Number of unique strings:\", unique_count)\n",
    "\n",
    "# a little data cleaning to ensure all elements were of the correct string type\n",
    "df_x[0] = df_x[0].apply(lambda x: x if isinstance(x, str) else str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tokenizer to learn vocabulary\n",
    "tokenizer = Tokenizer(num_words = unique_count, oov_token='_UNK_')\n",
    "tokenizer.fit_on_texts(df_x[0])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# using learned vocabulary to encode words to their distinct integer value\n",
    "encoded_df_x = tokenizer.texts_to_sequences(df_x[0])\n",
    "padded_encoded_df_x = pad_sequences(encoded_df_x, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be used for converting multiclassification dataset and converting to single classification datasets\n",
    "def split_data_by_label(df, label_num):\n",
    "    df_for_label = pd.DataFrame()\n",
    "\n",
    "    for index, elem in enumerate(df):\n",
    "        df_for_label.loc[index, 0] = int(elem == label_num)\n",
    "    \n",
    "    return df_for_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary to hold all of the one vs all datasets\n",
    "ova_datasets = {0:'', 1:'', 2:'', 3:'', 4:'', 5:'', 6:'', 7:''}\n",
    "\n",
    "for k,v in enumerate(ova_datasets):\n",
    "    ova_datasets[k] = split_data_by_label(encoded_df_y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(unique_count, max_len, act_func):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(800, 300, input_length=max_len))\n",
    "    model.add(SpatialDropout1D(0.1))\n",
    "    model.add(LSTM(32, dropout=0.1, recurrent_dropout=0.1,\n",
    "                   return_sequences=True, activation=act_func))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.1, activation=act_func)))\n",
    "    model.add(Dense(128, activation=act_func))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(256, activation=act_func))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation=act_func))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1, name='out_layer', activation=\"sigmoid\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(training_loss, val_loss, training_accuracy, val_accuracy, malware_type):\n",
    "    # Plot training and validation loss\n",
    "    plt.plot(training_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('results/' + malware_type + '_loss_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.plot(training_accuracy, label='Training Accuracy')\n",
    "    plt.plot(val_accuracy, label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig('results/' + malware_type + '_acc_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "# output of models will a floating point number so using a threshold all model output values will be converted to a binary label\n",
    "def convert_probabilities_to_labels(y_pred):\n",
    "    pred_labels = []\n",
    "\n",
    "    for i in y_pred:\n",
    "        if i >= 0.5:\n",
    "            pred_labels.append(1)\n",
    "        else:\n",
    "            pred_labels.append(0)\n",
    "            \n",
    "    return pred_labels\n",
    "    \n",
    "def train_and_evaluate_model(df_x, binary_y, malware_type, activation_func, oversample=False, plot=True):\n",
    "    # to help with an imbalanced data set creating an option to oversample on the minority class to create equal distributions\n",
    "    if oversample:\n",
    "        oversampler = RandomOverSampler()\n",
    "        df_x, binary_y = oversampler.fit_resample(df_x, binary_y)\n",
    "\n",
    "    # Split the data into training, validation, and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, binary_y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=75)\n",
    "\n",
    "    ova_model = create_lstm_model(unique_count, max_len, activation_func)\n",
    "    ova_model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "    # Configure early stopping\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "    # Train and evaluate model\n",
    "    history = ova_model.fit(X_train, y_train, epochs=10, batch_size=100, callbacks=[early_stopping], validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "    # Store training loss and accuracy\n",
    "    training_loss = history.history['loss']\n",
    "    training_accuracy = history.history['accuracy']\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    if plot:\n",
    "        # Plot training and validation loss\n",
    "        plot_loss_and_accuracy(training_loss, val_loss, training_accuracy, val_accuracy, malware_type)\n",
    "    \n",
    "    # test on unseen data and print confusion matrix from results\n",
    "    predict_labels_and_calculate_confusion_matrix(ova_model, X_test, y_test, malware_type, activation_func)\n",
    "\n",
    "def predict_labels_and_calculate_confusion_matrix(ova_model, X_test, y_test, malware_type, activation_func, save=True):\n",
    "    # Predict labels using the trained model\n",
    "    y_pred = ova_model.predict(X_test)\n",
    "    y_pred = convert_probabilities_to_labels(y_pred)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    if save:\n",
    "        save_confusion_matrices_with_metrics(cm, y_test, y_pred, malware_type, activation_func)\n",
    "\n",
    "def save_confusion_matrices_with_metrics(cm, y_test, y_pred, malware_type, activation_func):\n",
    "    # Generate timestamp\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Create filename with timestamp\n",
    "    filename = f\"results/confusion_matrices/confusion_matrices_{activation_func}_{timestamp}.txt\"\n",
    "\n",
    "    # Open the file in append mode\n",
    "    with open(filename, \"a\") as file:\n",
    "        # Print the confusion matrices and metrics\n",
    "        print(f'Confusion Matrix and Metrics for {malware_type} Model:\\n', file=file)\n",
    "        print(cm, file=file)\n",
    "\n",
    "        # Calculate metrics\n",
    "        report = classification_report(y_test, y_pred, target_names=[\"other\", malware_type])\n",
    "        print(report, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new model per classifier and for each train the model\n",
    "ova_models = {0:'', 1:'', 2:'', 3:'', 4:'', 5:'', 6:'', 7:''}\n",
    "activation_funcs = ['tanh', 'sigmoid', 'relu', 'softsign']\n",
    "\n",
    "for func in activation_funcs:\n",
    "    for k,v in enumerate(ova_models):\n",
    "        train_and_evaluate_model(padded_encoded_df_x, ova_datasets[k], inversed_mapping[k], func, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all all classes f1 scores for both posititive and negative classifications and do it for each activation function\n",
    "model_types = ['Adware', 'Backdoor', 'Downloader', 'Dropper', 'Spyware', 'Trojan', 'Virus', 'Worms']\n",
    "\n",
    "# Statically typed values from results. TODO to generate this in a more dynamic fashion\n",
    "softsign_positive_f1 = [0.97, 0.83, 0.87, 0.84, 0.82, 0.77, 0.88, 0.79]\n",
    "softsign_negative_f1 = [0.97, 0.80, 0.88, 0.82, 0.79, 0.75, 0.87, 0.79]\n",
    "\n",
    "relu_positive_f1 = [0.84, 0.73, 0.76, 0.71, 0.73, 0.68, 0.77, 0.73]\n",
    "relu_negative_f1 = [0.85, 0.72, 0.79, 0.66, 0.66, 0.66, 0.73, 0.74]\n",
    "\n",
    "sigmoid_positive_f1 = [0.88, 0.71, 0.74, 0.69, 0.70, 0.68, 0.79, 0.71]\n",
    "sigmoid_negative_f1 = [0.89, 0.73, 0.79, 0.71, 0.68, 0.70, 0.83, 0.73]\n",
    "\n",
    "tanh_positive_f1 = [0.96, 0.83, 0.87, 0.83, 0.84, 0.76, 0.86, 0.80]\n",
    "tanh_negative_f1 = [0.97, 0.83, 0.85, 0.81, 0.81, 0.76, 0.88, 0.79]\n",
    "\n",
    "x = np.arange(len(model_types))\n",
    "width = 0.35\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "# Plot for softsign activation function\n",
    "axs[0, 0].bar(x, softsign_positive_f1, width, label='Softsign Positive')\n",
    "axs[0, 0].bar(x + width, softsign_negative_f1, width, label='Softsign Negative')\n",
    "axs[0, 0].set_ylabel('F1 Score')\n",
    "axs[0, 0].set_title('Softsign Positive and Negative F1 Score')\n",
    "axs[0, 0].set_xticks(x)\n",
    "axs[0, 0].set_xticklabels(model_types, rotation=45, ha='right')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot for relu activation function\n",
    "axs[0, 1].bar(x, relu_positive_f1, width, label='ReLU Positive')\n",
    "axs[0, 1].bar(x + width, relu_negative_f1, width, label='ReLU Negative')\n",
    "axs[0, 1].set_ylabel('F1 Score')\n",
    "axs[0, 1].set_title('ReLU Positive and Negative F1 Score')\n",
    "axs[0, 1].set_xticks(x)\n",
    "axs[0, 1].set_xticklabels(model_types, rotation=45, ha='right')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot for sigmoid activation function\n",
    "axs[1, 0].bar(x, sigmoid_positive_f1, width, label='Sigmoid Positive')\n",
    "axs[1, 0].bar(x + width, sigmoid_negative_f1, width, label='Sigmoid Negative')\n",
    "axs[1, 0].set_ylabel('F1 Score')\n",
    "axs[1, 0].set_title('Sigmoid Positive and Negative F1 Score')\n",
    "axs[1, 0].set_xticks(x)\n",
    "axs[1, 0].set_xticklabels(model_types, rotation=45, ha='right')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot for tanh activation function\n",
    "axs[1, 1].bar(x, tanh_positive_f1, width, label='Tanh Positive')\n",
    "axs[1, 1].bar(x + width, tanh_negative_f1, width, label='Tanh Negative')\n",
    "axs[1, 1].set_ylabel('F1 Score')\n",
    "axs[1, 1].set_title('Tanh Positive and Negative F1 Score')\n",
    "axs[1, 1].set_xticks(x)\n",
    "axs[1, 1].set_xticklabels(model_types, rotation=45, ha='right')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
